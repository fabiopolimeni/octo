use crate::conversation::{Conversation, Role, State};

use std::{collections::HashMap, ops::AddAssign};

use anyhow::{anyhow, Result};
use reqwest::Client;
use reqwest_eventsource::{Event, EventSource};
use serde::{Deserialize, Serialize};

use async_trait::async_trait;
use tokio_stream::StreamExt;

#[derive(Deserialize, Debug)]
struct Logprob {
    token: String,
    logprob: f64,
    bytes: Vec<i64>,
}

#[derive(Deserialize, Debug)]
struct Content {
    token: String,
    logprob: f64,
    #[serde(skip_serializing_if = "Option::is_none")]
    bytes: Option<Vec<i64>>,
    top_logprobs: Vec<Logprob>,
}

#[derive(Deserialize, Debug)]
struct Logprobs {
    #[serde(skip_serializing_if = "Option::is_none")]
    content: Option<Vec<Content>>,
}

#[derive(Deserialize, Serialize, Debug, Clone)]
struct Function {
    name: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    arguments: Option<String>,

    #[serde(skip_serializing_if = "Option::is_none")]
    description: Option<String>,
}

#[derive(Deserialize, Serialize, Debug, Clone)]
struct ToolCall {
    id: String,

    #[serde(rename = "type")]
    type_: String,
    function: Function,
}

#[derive(Deserialize, Debug)]
struct Choice {
    #[serde(skip_serializing_if = "Option::is_none")]
    index: Option<i64>,

    #[serde(alias = "delta")]
    #[serde(alias = "message")]
    #[serde(skip_serializing_if = "Option::is_none")]
    reply: Option<Data>,

    #[serde(skip_serializing_if = "Option::is_none")]
    logprobs: Option<Logprobs>,

    #[serde(skip_serializing_if = "Option::is_none")]
    finish_reason: Option<String>,

    #[serde(skip_serializing_if = "Option::is_none")]
    text: Option<String>,
}

#[derive(Deserialize, Serialize, Debug, Clone)]
struct Data {
    /// The role of the author.
    #[serde(skip_serializing_if = "Option::is_none")]
    role: Option<String>,

    /// The content of the message.
    #[serde(skip_serializing_if = "Option::is_none")]
    content: Option<String>,

    /// An optional name for the participant. Provides the model information
    /// to differentiate between participants of the same role.
    #[serde(skip_serializing_if = "Option::is_none")]
    name: Option<String>,

    /// The tool calls generated by the model, such as function calls.
    #[serde(skip_serializing_if = "Option::is_none")]
    tool_calls: Option<Vec<ToolCall>>,

    /// Tool call that this message is responding to.
    #[serde(skip_serializing_if = "Option::is_none")]
    tool_call_id: Option<String>,
}

#[derive(Deserialize, Debug)]
struct Usage {
    prompt_tokens: i64,
    completion_tokens: i64,
    total_tokens: i64,
}

#[derive(Deserialize, Debug)]
struct Token {
    id: i64,
    text: String,
    logprob: f64,
    special: bool,
}

#[derive(Deserialize, Debug)]
struct ErrorObject {
    message: String,

    #[serde(rename = "type")]
    type_: String,

    #[serde(skip_serializing_if = "Option::is_none")]
    param: Option<String>,

    #[serde(skip_serializing_if = "Option::is_none")]
    code: Option<String>,
}

#[derive(Deserialize, Debug)]
#[serde(untagged)]
enum Response {
    Error {
        error: ErrorObject,
    },
    Completion {
        id: String,
        object: String,
        created: i64,
        choices: Vec<Choice>,

        #[serde(skip_serializing_if = "Option::is_none")]
        model: Option<String>,

        #[serde(skip_serializing_if = "Option::is_none")]
        usage: Option<Usage>,

        #[serde(skip_serializing_if = "Option::is_none")]
        system_fingerprint: Option<String>,

        #[serde(skip_serializing_if = "Option::is_none")]
        generated_text: Option<String>,

        #[serde(skip_serializing_if = "Option::is_none")]
        stats: Option<HashMap<String, i64>>,

        #[serde(skip_serializing_if = "Option::is_none")]
        token: Option<Token>,
    },
}

#[derive(Deserialize, Serialize, Debug)]
struct Tool {
    #[serde(rename = "type")]
    type_: String,
    function: Function,
}

#[derive(Deserialize, Serialize, Debug)]
struct ResponseFormat {
    #[serde(rename = "type")]
    type_: String,
}

#[derive(Deserialize, Serialize, Debug)]
struct Request {
    /// A list of messages comprising the conversation so far
    messages: Vec<Data>,

    /// The ID of the model to use for completion
    model: String,

    /// Number between -2.0 and 2.0. Positive values penalize
    /// new tokens based on their existing frequency in the text so far,
    /// decreasing the model's likelihood to repeat the same line verbatim.
    frequency_penalty: f64,

    /// Modify the likelihood of specified tokens appearing in the completion.
    /// Accepts a JSON object that maps tokens (specified by their token ID in
    /// the tokenizer) to an associated bias value from -100 to 100. Mathematically,
    /// the bias is added to the logits generated by the model prior to sampling.
    /// The exact effect will vary per model, but values between -1 and 1 should
    /// decrease or increase likelihood of selection; values like -100 or 100 should
    /// result in a ban or exclusive selection of the relevant token.
    #[serde(skip_serializing_if = "Option::is_none")]
    logit_bias: Option<HashMap<String, f64>>,

    /// Whether to return log probabilities of the output tokens or not.
    /// If true, returns the log probabilities of each output token returned
    /// in the content of message.
    logprobs: bool,

    /// An integer between 0 and 5 specifying the number of most likely tokens
    /// to return at each token position, each with an associated log probability.
    /// logprobs must be set to true if this parameter is used.
    #[serde(skip_serializing_if = "Option::is_none")]
    top_logprobs: Option<i64>,

    /// The maximum number of tokens to generate.
    max_tokens: i64,

    /// How many chat completion choices to generate for each input message.
    /// Note that you will be charged based on the number of generated tokens
    /// across all of the choices. Keep n as 1 to minimize costs.
    n: i64,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they
    /// appear in the text so far, increasing the model's likelihood to talk about new topics.
    presence_penalty: f64,

    /// An object specifying the format that the model must output.
    /// Setting to { "type": "json_object" } enables JSON mode, which guarantees the message
    /// the model generates is valid JSON.
    /// Important: when using JSON mode, you must also instruct the model to produce JSON
    /// yourself via a system or user message. Without this, the model may generate an unending
    /// stream of whitespace until the generation reaches the token limit, resulting in a
    /// long-running and seemingly "stuck" request.
    #[serde(skip_serializing_if = "Option::is_none")]
    response_format: Option<ResponseFormat>,

    /// This feature is in Beta. If specified, our system will make a best effort to sample
    /// deterministically, such that repeated requests with the same seed and parameters should
    /// return the same result. Determinism is not guaranteed, and you should refer to the
    /// system_fingerprint response parameter to monitor changes in the backend.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(alias = "random_seed")]
    seed: Option<i64>,

    /// Up to 4 sequences where the API will stop generating further tokens.
    #[serde(skip_serializing_if = "Option::is_none")]
    stop: Option<Vec<String>>,

    /// If set, partial message deltas will be sent. Tokens will be sent as data-only
    /// server-sent events as they become available, with the stream terminated by a data: [DONE]
    stream: bool,

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make
    /// the output more random, while lower values like 0.2 will make it more focused and
    /// deterministic. We generally recommend altering this or top_p but not both.
    temperature: f64,

    /// An alternative to sampling with temperature, called nucleus sampling, where the model
    /// considers the results of the tokens with top_p probability mass. So 0.1 means only
    /// the tokens comprising the top 10% probability mass are considered.
    /// We generally recommend altering this or temperature but not both.
    top_p: f64,

    /// The user ID to associate with this request.
    #[serde(skip_serializing_if = "Option::is_none")]
    user: Option<String>,

    /// A list of tools the model may call. Currently, only functions are supported as a tool.
    /// Use this to provide a list of functions the model may generate JSON inputs for.
    #[serde(skip_serializing_if = "Option::is_none")]
    tools: Option<Vec<Tool>>,

    /// Controls which (if any) function is called by the model. none means the model will not
    /// call a function and instead generates a message. auto means the model can pick between
    /// generating a message or calling a function.
    /// None is the default when no functions are present. auto is the default if functions are
    /// present.
    #[serde(skip_serializing_if = "Option::is_none")]
    tool_choice: Option<String>,
}

impl Default for Request {
    fn default() -> Self {
        Request {
            messages: vec![],
            model: "no-model".to_owned(),
            frequency_penalty: 0.0,
            logit_bias: None,
            logprobs: false,
            top_logprobs: None,
            max_tokens: 1024,
            n: 1,
            presence_penalty: 0.0,
            response_format: None,
            seed: None,
            stop: None,
            stream: false,
            temperature: 1.0,
            top_p: 1.0,
            user: None,
            tools: None,
            tool_choice: None,
        }
    }
}

#[derive(Debug, Clone)]
pub struct Settings {
    pub temperature: f64,
    pub max_tokens: i64,
    pub seed: Option<i64>,
    pub stream: bool,
}

impl Default for Settings {
    fn default() -> Self {
        Settings {
            temperature: 1.0,
            max_tokens: 1024,
            seed: None,
            stream: false,
        }
    }
}

pub struct Chat {
    client: Client,
    api_key: String,
    url: reqwest::Url,
    model: String,
    settings: Settings,
    history: Vec<Data>,
}

impl Chat {
    pub fn new(api_key: &str, url: &str, model: &str, settings: &Settings) -> Self {
        let client = Client::new();
        Chat {
            client,
            api_key: api_key.to_string(),
            url: url.parse().unwrap(),
            model: model.to_string(),
            settings: settings.clone(),
            history: vec![],
        }
    }
}

#[async_trait]
impl Conversation for Chat {
    fn build(&mut self, role: Role, message: &str) -> &mut Self {
        self.history.push(Data {
            role: Some(role.to_string()),
            content: Some(message.to_string()),
            name: None,
            tool_calls: None,
            tool_call_id: None,
        });

        self
    }

    async fn send<F>(&mut self, f: F) -> Result<()>
    where
        F: Fn(State) + Send,
    {
        // FIXME - We may pass all request fields by ref, instead of copying
        // all values, as this object will be serialized and sent through
        // network anyway, therefore, allocating all this memory just to drop
        // it at the end of this scope doesn't sound smart.
        let request = Request {
            messages: self.history.clone(),
            model: self.model.clone(),
            stream: true,
            ..Default::default()
        };

        // Make POST request
        let builder = self
            .client
            .post(self.url.clone())
            .header("Content-Type", "application/json")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&request);

        // We always request stream responses in order to keep the implementation
        // short, regardless of what the users asks. Though, the user can ask for
        // single message response, hence, we build a string out of chunk and return
        // that once the service is done replying.
        let mut text = String::new();

        let mut es = EventSource::new(builder).unwrap();
        while let Some(event) = es.next().await {
            match event {
                Ok(Event::Open) => {
                    f(State::Start);
                }
                Ok(Event::Message(message)) => {
                    //println!("Message: {:#?}", message);
                    let data_str = message.data.as_str();
                    if data_str.contains("[DONE]") {
                        let msg = text.clone();

                        // When we are done, send the text to the user, if stream is false
                        if !self.settings.stream {
                            f(State::Message(&msg));
                        }

                        // Add response to the history
                        self.history.push(Data {
                            role: Some(Role::Assistant.to_string()),
                            content: Some(msg),
                            name: None,
                            tool_calls: None,
                            tool_call_id: None,
                        });

                        f(State::Done);
                        es.close();
                        return Ok(());
                    } else {
                        match serde_json::from_str::<Response>(data_str)? {
                            Response::Error { error } => {
                                //eprintln!("Error caught: {}", error.message);
                                es.close();
                                return Err(anyhow!(error.message));
                            }
                            Response::Completion { choices, .. } => {
                                let choice = &choices[0];

                                // NOTE - Does this fail if not OpenAI?
                                let reply = choice.reply.as_ref().unwrap();

                                if reply.content.is_some() {
                                    let chunk = reply.content.as_ref().unwrap();

                                    // println!("{}", content.unwrap());
                                    text.add_assign(chunk);

                                    // Only send message chunks if we are streaming
                                    if self.settings.stream {
                                        f(State::Message(chunk));
                                    }
                                } else if choice.finish_reason.is_some() {
                                    // FIXME - Interpret finish_reason
                                    f(State::Stop);
                                }
                            }
                        }
                    }
                }
                Err(error) => {
                    //eprintln!("Error caught: {}", error);
                    es.close();
                    return Err(anyhow!(error.to_string()));
                }
            }
        }

        return Ok(());
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_chat_request() {
        vec![
            Data {
                role: Some(Role::User.to_string()),
                content: Some("Hello".to_string()),
                name: None,
                tool_calls: None,
                tool_call_id: None,
            },
            Data {
                role: Some(Role::User.to_string()),
                content: Some("How are you?".to_string()),
                name: None,
                tool_calls: None,
                tool_call_id: None,
            },
        ];
        let model = "gpt-3.5-turbo";
        let url = "https://api.openai.com/v1/engines/davinci-codex/completions";
        let api_key = "YOUR_API_KEY";
        let settings = Settings::default();

        let mut chat = Chat::new(api_key, url, model, &settings);

        let result = chat.send(|_| {}).await;

        assert!(result.is_err(), "{result:?}");
    }
}
