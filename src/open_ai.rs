use crate::chat::{Chat, Role, What};

use std::{collections::HashMap, ops::AddAssign};

use anyhow::{anyhow, Result};
use reqwest::Client;
use reqwest_eventsource::{Event, EventSource};
use serde::{Deserialize, Serialize};
use serde_json;

use async_trait::async_trait;
use tokio_stream::StreamExt;

#[derive(Deserialize, Debug)]
struct Logprob {
    token: String,
    logprob: f64,
    bytes: Vec<i64>,
}

#[derive(Deserialize, Debug)]
struct Content {
    token: String,
    logprob: f64,
    #[serde(skip_serializing_if = "Option::is_none")]
    bytes: Option<Vec<i64>>,
    top_logprobs: Vec<Logprob>,
}

#[derive(Deserialize, Debug)]
struct Logprobs {
    #[serde(skip_serializing_if = "Option::is_none")]
    content: Option<Vec<Content>>,
}

#[derive(Deserialize, Serialize, Debug)]
struct Function {
    name: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    arguments: Option<String>,

    #[serde(skip_serializing_if = "Option::is_none")]
    description: Option<String>,
}

#[derive(Deserialize, Serialize, Debug)]
struct ToolCall {
    id: String,

    #[serde(rename = "type")]
    type_: String,
    function: Function,
}

#[derive(Deserialize, Debug)]
struct Choice {
    index: i64,

    #[serde(alias = "delta")]
    #[serde(alias = "message")]
    reply: Reply,

    #[serde(skip_serializing_if = "Option::is_none")]
    logprobs: Option<Logprobs>,

    #[serde(skip_serializing_if = "Option::is_none")]
    finish_reason: Option<String>,
}

#[derive(Deserialize, Serialize, Debug)]
struct Reply {
    /// The role of the messages author
    #[serde(skip_serializing_if = "Option::is_none")]
    role: Option<String>,

    /// The content of the message.
    #[serde(skip_serializing_if = "Option::is_none")]
    content: Option<String>,

    /// An optional name for the participant. Provides the model information
    /// to differentiate between participants of the same role.
    #[serde(skip_serializing_if = "Option::is_none")]
    name: Option<String>,

    /// The tool calls generated by the model, such as function calls.
    #[serde(skip_serializing_if = "Option::is_none")]
    tool_calls: Option<Vec<ToolCall>>,

    /// Tool call that this message is responding to.
    #[serde(skip_serializing_if = "Option::is_none")]
    tool_call_id: Option<String>,
}

#[derive(Deserialize, Debug)]
struct Usage {
    prompt_tokens: i64,
    completion_tokens: i64,
    total_tokens: i64,
}

#[derive(Deserialize, Debug)]
struct ErrorObject {
    message: String,

    #[serde(rename = "type")]
    type_: String,

    #[serde(skip_serializing_if = "Option::is_none")]
    param: Option<String>,

    #[serde(skip_serializing_if = "Option::is_none")]
    code: Option<String>,
}

#[derive(Deserialize, Debug)]
#[serde(untagged)]
enum Response {
    Error {
        error: ErrorObject,
    },
    Completion {
        id: String,
        object: String,
        created: i64,
        model: String,
        choices: Vec<Choice>,

        #[serde(skip_serializing_if = "Option::is_none")]
        usage: Option<Usage>,

        #[serde(skip_serializing_if = "Option::is_none")]
        system_fingerprint: Option<String>,
    },
}

#[derive(Deserialize, Serialize, Debug)]
struct Tool {
    #[serde(rename = "type")]
    type_: String,
    function: Function,
}

#[derive(Deserialize, Serialize, Debug)]
struct ResponseFormat {
    #[serde(rename = "type")]
    type_: String,
}

#[derive(Deserialize, Serialize, Debug)]
struct Request {
    /// A list of messages comprising the conversation so far
    messages: Vec<Reply>,

    /// The ID of the model to use for completion
    model: String,

    /// Number between -2.0 and 2.0. Positive values penalize
    /// new tokens based on their existing frequency in the text so far,
    /// decreasing the model's likelihood to repeat the same line verbatim.
    frequency_penalty: f64,

    /// Modify the likelihood of specified tokens appearing in the completion.
    /// Accepts a JSON object that maps tokens (specified by their token ID in
    /// the tokenizer) to an associated bias value from -100 to 100. Mathematically,
    /// the bias is added to the logits generated by the model prior to sampling.
    /// The exact effect will vary per model, but values between -1 and 1 should
    /// decrease or increase likelihood of selection; values like -100 or 100 should
    /// result in a ban or exclusive selection of the relevant token.
    #[serde(skip_serializing_if = "Option::is_none")]
    logit_bias: Option<HashMap<String, f64>>,

    /// Whether to return log probabilities of the output tokens or not.
    /// If true, returns the log probabilities of each output token returned
    /// in the content of message.
    logprobs: bool,

    /// An integer between 0 and 5 specifying the number of most likely tokens
    /// to return at each token position, each with an associated log probability.
    /// logprobs must be set to true if this parameter is used.
    #[serde(skip_serializing_if = "Option::is_none")]
    top_logprobs: Option<i64>,

    /// The maximum number of tokens to generate.
    max_tokens: i64,

    /// How many chat completion choices to generate for each input message.
    /// Note that you will be charged based on the number of generated tokens
    /// across all of the choices. Keep n as 1 to minimize costs.
    n: i64,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they
    /// appear in the text so far, increasing the model's likelihood to talk about new topics.
    presence_penalty: f64,

    /// An object specifying the format that the model must output.
    /// Setting to { "type": "json_object" } enables JSON mode, which guarantees the message
    /// the model generates is valid JSON.
    /// Important: when using JSON mode, you must also instruct the model to produce JSON
    /// yourself via a system or user message. Without this, the model may generate an unending
    /// stream of whitespace until the generation reaches the token limit, resulting in a
    /// long-running and seemingly "stuck" request.
    #[serde(skip_serializing_if = "Option::is_none")]
    response_format: Option<ResponseFormat>,

    /// This feature is in Beta. If specified, our system will make a best effort to sample
    /// deterministically, such that repeated requests with the same seed and parameters should
    /// return the same result. Determinism is not guaranteed, and you should refer to the
    /// system_fingerprint response parameter to monitor changes in the backend.
    #[serde(skip_serializing_if = "Option::is_none")]
    seed: Option<i64>,

    /// Up to 4 sequences where the API will stop generating further tokens.
    #[serde(skip_serializing_if = "Option::is_none")]
    stop: Option<Vec<String>>,

    /// If set, partial message deltas will be sent. Tokens will be sent as data-only
    /// server-sent events as they become available, with the stream terminated by a data: [DONE]
    stream: bool,

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make
    /// the output more random, while lower values like 0.2 will make it more focused and
    /// deterministic. We generally recommend altering this or top_p but not both.
    temperature: f64,

    /// An alternative to sampling with temperature, called nucleus sampling, where the model
    /// considers the results of the tokens with top_p probability mass. So 0.1 means only
    /// the tokens comprising the top 10% probability mass are considered.
    /// We generally recommend altering this or temperature but not both.
    top_p: f64,

    /// The user ID to associate with this request.
    #[serde(skip_serializing_if = "Option::is_none")]
    user: Option<String>,

    /// A list of tools the model may call. Currently, only functions are supported as a tool.
    /// Use this to provide a list of functions the model may generate JSON inputs for.
    #[serde(skip_serializing_if = "Option::is_none")]
    tools: Option<Vec<Tool>>,

    /// Controls which (if any) function is called by the model. none means the model will not
    /// call a function and instead generates a message. auto means the model can pick between
    /// generating a message or calling a function.
    /// none is the default when no functions are present. auto is the default if functions are
    /// present.
    #[serde(skip_serializing_if = "Option::is_none")]
    tool_choice: Option<String>,
}

impl Default for Request {
    fn default() -> Self {
        Request {
            messages: vec![],
            model: "gpt-3.5-turbo".to_owned(),
            frequency_penalty: 0.0,
            logit_bias: None,
            logprobs: false,
            top_logprobs: None,
            max_tokens: 1024,
            n: 1,
            presence_penalty: 0.0,
            response_format: None,
            seed: None,
            stop: None,
            stream: false,
            temperature: 1.0,
            top_p: 1.0,
            user: None,
            tools: None,
            tool_choice: None,
        }
    }
}

pub struct OpenAI {
    client: Client,
    url: reqwest::Url,
    model: String,
}

impl OpenAI {
    pub fn new(url: &str, model: &str) -> Self {
        let client = Client::new();
        OpenAI {
            client,
            url: url.parse().unwrap(),
            model: model.to_string(),
        }
    }
}

#[async_trait]
impl Chat for OpenAI {
    async fn message(&mut self, role: Role, message: &str) -> Result<String> {
        let request = Request {
            messages: vec![Reply {
                role: Some(role.to_string().to_lowercase()),
                content: Some(message.to_string()),
                name: None,
                tool_calls: None,
                tool_call_id: None,
            }],
            model: self.model.clone(),
            ..Default::default()
        };

        let bearer_token = std::env::var("OPENAI_API_KEY")?;

        // Make POST request
        let builder = self
            .client
            .post(self.url.clone())
            .header("Content-Type", "application/json")
            .header("Authorization", format!("Bearer {}", bearer_token))
            .json(&request);

        let response = builder.send().await?;

        // Return response body
        match response.json::<Response>().await? {
            Response::Error { error } => Err(anyhow::anyhow!(error.message)),
            Response::Completion { choices, .. } => {
                let content = choices[0].reply.content.clone().unwrap();
                Ok(content)
            }
        }
    }

    async fn stream<F>(&mut self, role: Role, message: &str, f: F) -> Result<()>
    where
        F: Fn(&str, What) + Send,
    {
        let request = Request {
            messages: vec![Reply {
                role: Some(role.to_string()),
                content: Some(message.to_string()),
                name: None,
                tool_calls: None,
                tool_call_id: None,
            }],
            stream: true,
            model: self.model.clone(),
            ..Default::default()
        };

        let bearer_token = std::env::var("OPENAI_API_KEY")?;

        // Make POST request
        let builder = self
            .client
            .post(self.url.clone())
            .header("Content-Type", "application/json")
            .header("Authorization", format!("Bearer {}", bearer_token))
            .json(&request);

        let mut text = String::new();

        let mut es = EventSource::new(builder).unwrap();
        while let Some(event) = es.next().await {
            match event {
                Ok(Event::Open) => {
                    f("", What::Start);
                }
                Ok(Event::Message(message)) => {
                    //println!("Message: {:#?}", message);
                    let data_str = message.data.as_str();
                    if data_str.contains("[DONE]") {
                        f("", What::Done);
                        es.close();
                        return Ok(());
                    } else {
                        match serde_json::from_str::<Response>(data_str).unwrap() {
                            Response::Error { error } => {
                                //eprintln!("Error: {}", error.message);
                                es.close();
                                return Err(anyhow!(error.message));
                            }
                            Response::Completion { choices, .. } => {
                                let choice = &choices[0];
                                let content = choice.reply.content.clone();
                                if content.is_some() {
                                    let chunk = &content.unwrap();
                                    // println!("{}", content.unwrap());
                                    text.add_assign(chunk);
                                    f(chunk.as_str(), What::Chunk);
                                } else if choice.finish_reason.is_some() {
                                    // FIXME: Interpret finish_reason
                                    f("", What::Stop);
                                }
                            }
                        }
                    }
                }
                Err(error) => {
                    //eprintln!("Error caught: {}", error);
                    es.close();
                    return Err(anyhow!(error.to_string()));
                }
            }
        }

        return Ok(());
    }
}
